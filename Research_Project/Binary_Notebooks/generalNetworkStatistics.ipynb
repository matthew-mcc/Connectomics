{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First, get access to null model (ASD)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.61734693877551\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Example observed correlation matrix (randomly generated for demonstration)\n",
    "import networkx as nx\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_path_10 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\10%\\Matrices\\Caltech_0051456_rois_cc400_mat.txt'\n",
    "file_path_15 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\15%\\Matrices\\Caltech_0051456_rois_cc400_mat.txt'\n",
    "file_path_20 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\20%\\Matrices\\Caltech_0051456_rois_cc400_mat.txt'\n",
    "file_path_25 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\25%\\Matrices\\Caltech_0051456_rois_cc400_mat.txt'\n",
    "file_path_30 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\30%\\Matrices\\Caltech_0051456_rois_cc400_mat.txt'\n",
    "\n",
    "observed_matrix_10 = np.loadtxt(file_path_10)\n",
    "observed_matrix_15 = np.loadtxt(file_path_15)\n",
    "observed_matrix_20 = np.loadtxt(file_path_20)\n",
    "observed_matrix_25 = np.loadtxt(file_path_25)\n",
    "observed_matrix_30 = np.loadtxt(file_path_30)\n",
    "\n",
    "G_O_10 = nx.from_numpy_array(observed_matrix_10)\n",
    "\n",
    "\n",
    "# averageDegree = np.mean([g.degree(node) for node in g])\n",
    "G_O_15 = nx.from_numpy_array(observed_matrix_15)\n",
    "G_O_20 = nx.from_numpy_array(observed_matrix_20)\n",
    "G_O_25 = nx.from_numpy_array(observed_matrix_25)\n",
    "G_O_30 = nx.from_numpy_array(observed_matrix_30)\n",
    "\n",
    "print(np.mean([G_O_30.degree(node) for node in G_O_30]))\n",
    "\n",
    "graphs_10 = []\n",
    "graphs_15 = []\n",
    "graphs_20 = []\n",
    "graphs_25 = []\n",
    "graphs_30 = []\n",
    "for i in range(5):\n",
    "    graph_10 = copy.deepcopy(G_O_10)\n",
    "    graph_15 = copy.deepcopy(G_O_15)\n",
    "    graph_20 = copy.deepcopy(G_O_20)\n",
    "    graph_25 = copy.deepcopy(G_O_25)\n",
    "    graph_30 = copy.deepcopy(G_O_30)\n",
    "\n",
    "    G_R_10 = nx.random_reference(graph_10, 10, connectivity=False)\n",
    "    G_R_15 = nx.random_reference(graph_15, 10, connectivity=False)\n",
    "    G_R_20 = nx.random_reference(graph_20, 10, connectivity=False)\n",
    "    G_R_25 = nx.random_reference(graph_25, 10, connectivity=False)\n",
    "    G_R_30 = nx.random_reference(graph_30, 10, connectivity=False)\n",
    "    \n",
    "    graphs_10.append(G_R_10)\n",
    "    graphs_15.append(G_R_15)\n",
    "    graphs_20.append(G_R_20)\n",
    "    graphs_25.append(G_R_25)\n",
    "    graphs_30.append(G_R_30)\n",
    "\n",
    "    print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Example observed correlation matrix (randomly generated for demonstration)\n",
    "import networkx as nx\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_path_10 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\10%\\Matrices\\Caltech_0051476_rois_cc400_mat.txt'\n",
    "file_path_15 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\15%\\Matrices\\Caltech_0051476_rois_cc400_mat.txt'\n",
    "file_path_20 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\20%\\Matrices\\Caltech_0051476_rois_cc400_mat.txt'\n",
    "file_path_25 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\25%\\Matrices\\Caltech_0051476_rois_cc400_mat.txt'\n",
    "file_path_30 = R'C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\30%\\Matrices\\Caltech_0051476_rois_cc400_mat.txt'\n",
    "\n",
    "observed_matrix_10 = np.loadtxt(file_path_10)\n",
    "observed_matrix_15 = np.loadtxt(file_path_15)\n",
    "observed_matrix_20 = np.loadtxt(file_path_20)\n",
    "observed_matrix_25 = np.loadtxt(file_path_25)\n",
    "observed_matrix_30 = np.loadtxt(file_path_30)\n",
    "\n",
    "G_O_10 = nx.from_numpy_array(observed_matrix_10)\n",
    "G_O_15 = nx.from_numpy_array(observed_matrix_15)\n",
    "G_O_20 = nx.from_numpy_array(observed_matrix_20)\n",
    "G_O_25 = nx.from_numpy_array(observed_matrix_25)\n",
    "G_O_30 = nx.from_numpy_array(observed_matrix_30)\n",
    "\n",
    "graphs_10 = []\n",
    "graphs_15 = []\n",
    "graphs_20 = []\n",
    "graphs_25 = []\n",
    "graphs_30 = []\n",
    "for i in range(5):\n",
    "    graph_10 = copy.deepcopy(G_O_10)\n",
    "    graph_15 = copy.deepcopy(G_O_15)\n",
    "    graph_20 = copy.deepcopy(G_O_20)\n",
    "    graph_25 = copy.deepcopy(G_O_25)\n",
    "    graph_30 = copy.deepcopy(G_O_30)\n",
    "\n",
    "    G_R_10 = nx.random_reference(graph_10, 10, connectivity=False)\n",
    "    G_R_15 = nx.random_reference(graph_15, 10, connectivity=False)\n",
    "    G_R_20 = nx.random_reference(graph_20, 10, connectivity=False)\n",
    "    G_R_25 = nx.random_reference(graph_25, 10, connectivity=False)\n",
    "    G_R_30 = nx.random_reference(graph_30, 10, connectivity=False)\n",
    "    \n",
    "    graphs_10.append(G_R_10)\n",
    "    graphs_15.append(G_R_15)\n",
    "    graphs_20.append(G_R_20)\n",
    "    graphs_25.append(G_R_25)\n",
    "    graphs_30.append(G_R_30)\n",
    "\n",
    "    print(\"done!\")\n",
    "    \n",
    "all_random_graphs_nt = [graphs_10, graphs_15, graphs_20, graphs_25, graphs_30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PercentageThresholding(correlationMatrix, percentageToKeep):\n",
    "    '''\n",
    "    Function to apply a percentage based thresholding, returns new adjacency matrix\n",
    "    '''\n",
    "    \n",
    "    # Apply Fisher z-transformation to the correlation matrix\n",
    "    transformedMatrix = np.arctanh(correlationMatrix)\n",
    "    \n",
    "    # Flatten the matrix, sort the values in descending order\n",
    "    sortedValues = np.sort(transformedMatrix.flatten())[::-1]\n",
    "    \n",
    "    # Determine the thresholded value based on percentage (how many elements to keep)\n",
    "    numElementsToKeep = int(len(sortedValues) * percentageToKeep)\n",
    "    thresholdValue = sortedValues[numElementsToKeep - 1]\n",
    "    \n",
    "    # Create a new adjacency matrix based on our thresholding\n",
    "    adjMatrix = np.zeros(transformedMatrix.shape, dtype=int) # int for binary\n",
    "    adjMatrix[transformedMatrix >= thresholdValue] = 1 # 1 if keeping, 0 if thresholded\n",
    "    \n",
    "\n",
    "    \n",
    "    return adjMatrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117.60204081632654\n",
      "0.5410932385374854\n",
      "0.780665396780677\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hqs_random_correlation_matrix(observed_matrix):\n",
    "    \"\"\"\n",
    "    Generate a random correlation matrix using the HQS algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - observed_matrix: ndarray\n",
    "        The observed covariance matrix (N x N) to match.\n",
    "\n",
    "    Returns:\n",
    "    - correlation_matrix: ndarray\n",
    "        A random correlation matrix (N x N).\n",
    "    \"\"\"\n",
    "    # Ensure the observed matrix is a NumPy array\n",
    "    observed_matrix = np.array(observed_matrix)\n",
    "    \n",
    "    # Step 0: Extract properties of the observed matrix\n",
    "    N = observed_matrix.shape[0]\n",
    "    off_diag_elements = observed_matrix[np.triu_indices(N, k=1)]\n",
    "    e_off_diag = np.mean(off_diag_elements)  # Mean of off-diagonal elements\n",
    "    v_off_diag = np.var(off_diag_elements)  # Variance of off-diagonal elements\n",
    "\n",
    "    # Step 1: Calculate m\n",
    "    m = max(2, (e_off_diag**2) / (e_off_diag**2 - v_off_diag))\n",
    "\n",
    "    # Step 2: Calculate μ\n",
    "    mu = np.sqrt(e_off_diag / m)\n",
    "\n",
    "    # Step 3: Calculate σ²\n",
    "    sigma2 = -mu**2 + np.sqrt(mu**4 + v_off_diag / m)\n",
    "\n",
    "    # Step 4: Generate xi,j ~ N(μ, σ²)\n",
    "    xi = np.random.normal(loc=mu, scale=np.sqrt(sigma2), size=(N, int(m)))\n",
    "\n",
    "    # Step 5: Construct X matrix\n",
    "    X = xi\n",
    "\n",
    "    # Step 6: Compute covariance matrix C = XX^T\n",
    "    covariance_matrix = np.dot(X, X.T)\n",
    "\n",
    "    # Normalize to produce a correlation matrix\n",
    "    std_devs = np.sqrt(np.diag(covariance_matrix))\n",
    "    correlation_matrix = covariance_matrix / np.outer(std_devs, std_devs)\n",
    "\n",
    "    # Ensure diagonal is exactly 1\n",
    "    np.fill_diagonal(correlation_matrix, 1.0)\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "\n",
    "random_mat = hqs_random_correlation_matrix(observed_matrix_30)\n",
    "correlationMatrix = np.corrcoef(random_mat.T) # need to transpose (I think for all?)\n",
    "np.fill_diagonal(correlationMatrix, 0) # remove leading zeros\n",
    "\n",
    "adjMat = PercentageThresholding(correlationMatrix, 0.3)\n",
    "\n",
    "testG = nx.from_numpy_array(adjMat)\n",
    "averageDegree = np.mean([testG.degree(node) for node in testG])\n",
    "print(averageDegree)\n",
    "print(nx.global_efficiency(testG))\n",
    "print(nx.average_clustering(testG))\n",
    "# print(random_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"master_data_cc400.csv\")\n",
    "\n",
    "filtered_data = data[data['Age'] >= 18]\n",
    "filtered_data = filtered_data[filtered_data['Age'] <= 30]\n",
    "\n",
    "filtered_data_asd = filtered_data[filtered_data['DX_Group'] == 1]\n",
    "filtered_data_nt = filtered_data[filtered_data['DX_Group'] == 2]\n",
    "\n",
    "condensed_df_asd = filtered_data_asd.drop_duplicates(subset=[\"Participant ID\"], keep=\"first\")\n",
    "condensed_df_nt = filtered_data_nt.drop_duplicates(subset=[\"Participant ID\"], keep=\"first\")\n",
    "print(len(condensed_df_asd))\n",
    "print(len(condensed_df_nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Clustering ASD: 0.5499346478101116\n",
      "Avg Clustering NT: 0.5592567572158677\n",
      "Avg Clustering R - ASD: 0.33785238944605434\n",
      "Avg Clustering R - NT: 0.36041455593588523\n",
      "Global Efficiency ASD: 0.5363849558963651\n",
      "Global Efficiency NT: 0.5237616351841878\n",
      "Global Efficiency R - ASD: 0.5621568083754998\n",
      "Global Efficiency R - NT: 0.5776754092941317\n",
      "Average Degree R - ASD: 64.61479591836735\n",
      "Average Degree R - NT: 74.41530612244898\n",
      "Average Degree ASD: 22.59944722295341\n",
      "Average Degree NT: 22.688419808787195\n",
      "Avg Clustering R - ASD (std): 0.055815525809818835\n",
      "Avg Clustering R - NT (std): 0.06729415846862659\n",
      "Global Efficiency R - ASD (std): 0.04385015809788087\n",
      "Global Efficiency R - NT (std): 0.0500157674144302\n",
      "Average Degree R - ASD (std): 21.913466209195224\n",
      "Average Degree R - NT (std): 27.719307388217075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n10%\\nAverage Degree NT: 11.215114405414113\\nAverage Degree ASD: 11.126414846012107\\n\\n15%\\nAverage Degree NT: 16.912342893973577\\nAverage Degree ASD: 16.83107396683338\\n\\n20%\\nAverage Degree NT: 22.670533891932536\\nAverage Degree ASD: 22.577520400105286\\n\\n25%\\nAverage Degree NT: 28.42754323772693\\nAverage Degree ASD: 28.334561726770204\\n\\n30%\\nAverage Degree NT: 34.216564614888824\\nAverage Degree ASD: 34.12766517504606\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have 60 random graphs we can average metrics on!\n",
    "\n",
    "all_random_graphs = [graphs_10, graphs_15, graphs_20, graphs_25] # asd\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# 1. Average Clustering\n",
    "\n",
    "avg_clustering_r = 0\n",
    "avg_clusterings_r = []\n",
    "\n",
    "for thresh in all_random_graphs:\n",
    "    \n",
    "    # For each g in our thresholding value\n",
    "    avg = np.mean([nx.average_clustering(g) for g in thresh])\n",
    "    # print(avg)\n",
    "    avg_clusterings_r.append(avg)\n",
    "    \n",
    "\n",
    "avg_clustering_r_nt = 0\n",
    "avg_clusterings_r_nt = []\n",
    "for thresh in all_random_graphs_nt:\n",
    "    avg = np.mean([nx.average_clustering(g) for g in thresh])\n",
    "    avg_clusterings_r_nt.append(avg)\n",
    "\n",
    "avg_clustering_asd = condensed_df_asd['Average Clustering'].mean()\n",
    "avg_clustering_nt = condensed_df_nt['Average Clustering'].mean()\n",
    "avg_clustering_r = np.mean(avg_clusterings_r)\n",
    "avg_clustering_r_nt = np.mean(avg_clusterings_r_nt)\n",
    "\n",
    "print(f\"Avg Clustering ASD: {avg_clustering_asd}\")\n",
    "print(f\"Avg Clustering NT: {avg_clustering_nt}\")\n",
    "print(f\"Avg Clustering R - ASD: {avg_clustering_r}\")\n",
    "print(f\"Avg Clustering R - NT: {avg_clustering_r_nt}\")\n",
    "\n",
    "# 2. Global Efficiency\n",
    "\n",
    "global_effs = []\n",
    "\n",
    "for thresh in all_random_graphs:\n",
    "    \n",
    "    # For each g in our thresholding value\n",
    "    avg = np.mean([nx.global_efficiency(g) for g in thresh])\n",
    "    # print(avg)\n",
    "    global_effs.append(avg)\n",
    "    \n",
    "global_effs_nt = []\n",
    "\n",
    "for thresh in all_random_graphs_nt:\n",
    "    \n",
    "    # For each g in our thresholding value\n",
    "    avg = np.mean([nx.global_efficiency(g) for g in thresh])\n",
    "    # print(avg)\n",
    "    global_effs_nt.append(avg)\n",
    "    \n",
    "\n",
    "global_eff_asd = condensed_df_asd['Global Efficiency'].mean()\n",
    "global_eff_nt = condensed_df_nt['Global Efficiency'].mean()\n",
    "global_eff_r = np.mean(global_effs)\n",
    "global_eff_r_nt = np.mean(global_effs_nt)\n",
    "\n",
    "print(f\"Global Efficiency ASD: {global_eff_asd}\")\n",
    "print(f\"Global Efficiency NT: {global_eff_nt}\")\n",
    "print(f\"Global Efficiency R - ASD: {global_eff_r}\")\n",
    "print(f\"Global Efficiency R - NT: {global_eff_r_nt}\")\n",
    "\n",
    "# 3. Average Degree\n",
    "\n",
    "avg_degree = []\n",
    "for thresh in all_random_graphs:\n",
    "    # averageDegree = np.mean([G.degree(node) for node in G])\n",
    "    # For each g in our thresholding value\n",
    "    deg = []\n",
    "    for g in thresh:\n",
    "        averageDegree = np.mean([g.degree(node) for node in g])\n",
    "        deg.append(averageDegree)\n",
    "    \n",
    "    # avg = np.mean([nx.average(g) for g in thresh])\n",
    "    avg = np.mean(deg)\n",
    "    # print(avg)\n",
    "    avg_degree.append(avg)\n",
    "    \n",
    "\n",
    "avg_degree_nt = []\n",
    "for thresh in all_random_graphs_nt:\n",
    "    # averageDegree = np.mean([G.degree(node) for node in G])\n",
    "    # For each g in our thresholding value\n",
    "    deg = []\n",
    "    for g in thresh:\n",
    "        averageDegree = np.mean([g.degree(node) for node in g])\n",
    "        deg.append(averageDegree)\n",
    "    \n",
    "    # avg = np.mean([nx.average(g) for g in thresh])\n",
    "    avg = np.mean(deg)\n",
    "    # print(avg)\n",
    "    avg_degree_nt.append(avg)\n",
    "\n",
    "\n",
    "\n",
    "nt_degs = [11.215114405414113, 16.912342893973577, 22.670533891932536, 28.42754323772693, 34.216564614888824]\n",
    "asd_degs = [11.126414846012107, 16.83107396683338, 22.577520400105286, 28.334561726770204, 34.12766517504606]\n",
    "# print(np.mean(avg_degree))\n",
    "avg_degree_r = np.mean(avg_degree)\n",
    "avg_degree_r_nt = np.mean(avg_degree_nt)\n",
    "print(f\"Average Degree R - ASD: {avg_degree_r}\")\n",
    "print(f\"Average Degree R - NT: {avg_degree_r_nt}\")\n",
    "print(f\"Average Degree ASD: {np.mean(asd_degs)}\")\n",
    "print(f\"Average Degree NT: {np.mean(nt_degs)}\")\n",
    "\n",
    "# Example for Average Clustering\n",
    "avg_clustering_r_std = np.std(avg_clusterings_r)\n",
    "avg_clustering_r_nt_std = np.std(avg_clusterings_r_nt)\n",
    "\n",
    "print(f\"Avg Clustering R - ASD (std): {avg_clustering_r_std}\")\n",
    "print(f\"Avg Clustering R - NT (std): {avg_clustering_r_nt_std}\")\n",
    "\n",
    "# Example for Global Efficiency\n",
    "global_eff_r_std = np.std(global_effs)\n",
    "global_eff_r_nt_std = np.std(global_effs_nt)\n",
    "\n",
    "print(f\"Global Efficiency R - ASD (std): {global_eff_r_std}\")\n",
    "print(f\"Global Efficiency R - NT (std): {global_eff_r_nt_std}\")\n",
    "\n",
    "# Example for Average Degree\n",
    "avg_degree_r_std = np.std(avg_degree)\n",
    "avg_degree_r_nt_std = np.std(avg_degree_nt)\n",
    "\n",
    "print(f\"Average Degree R - ASD (std): {avg_degree_r_std}\")\n",
    "print(f\"Average Degree R - NT (std): {avg_degree_r_nt_std}\")\n",
    "\n",
    "'''\n",
    "10%\n",
    "Average Degree NT: 11.215114405414113\n",
    "Average Degree ASD: 11.126414846012107\n",
    "\n",
    "15%\n",
    "Average Degree NT: 16.912342893973577\n",
    "Average Degree ASD: 16.83107396683338\n",
    "\n",
    "20%\n",
    "Average Degree NT: 22.670533891932536\n",
    "Average Degree ASD: 22.577520400105286\n",
    "\n",
    "25%\n",
    "Average Degree NT: 28.42754323772693\n",
    "Average Degree ASD: 28.334561726770204\n",
    "\n",
    "30%\n",
    "Average Degree NT: 34.216564614888824\n",
    "Average Degree ASD: 34.12766517504606\n",
    "'''\n",
    "\n",
    "\n",
    "# 4. Degree Distribution (this will be tricky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Last File Parsed: Caltech_0051456_rois_aal_mat.txt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:21: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:21: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\mattm\\AppData\\Local\\Temp\\ipykernel_35052\\2395763997.py:21: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  matrix = np.loadtxt(f\"{matrixDirectory}\\{matrixFileName}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counter: 10, Last File Parsed: Caltech_0051466_rois_aal_mat.txt\n",
      "Counter: 20, Last File Parsed: Caltech_0051477_rois_aal_mat.txt\n",
      "Counter: 30, Last File Parsed: Caltech_0051487_rois_aal_mat.txt\n",
      "Counter: 40, Last File Parsed: CMU_b_0050657_rois_aal_mat.txt\n",
      "Counter: 50, Last File Parsed: KKI_0050780_rois_aal_mat.txt\n",
      "Counter: 60, Last File Parsed: KKI_0050791_rois_aal_mat.txt\n",
      "Counter: 70, Last File Parsed: KKI_0050814_rois_aal_mat.txt\n",
      "Counter: 80, Last File Parsed: KKI_0050825_rois_aal_mat.txt\n",
      "Counter: 90, Last File Parsed: Leuven_1_0050692_rois_aal_mat.txt\n",
      "Counter: 100, Last File Parsed: Leuven_1_0050702_rois_aal_mat.txt\n",
      "Counter: 110, Last File Parsed: Leuven_2_0050722_rois_aal_mat.txt\n",
      "Counter: 120, Last File Parsed: Leuven_2_0050733_rois_aal_mat.txt\n",
      "Counter: 130, Last File Parsed: Leuven_2_0050745_rois_aal_mat.txt\n",
      "Counter: 140, Last File Parsed: Leuven_2_0050756_rois_aal_mat.txt\n",
      "Counter: 150, Last File Parsed: MaxMun_a_0051370_rois_aal_mat.txt\n",
      "Counter: 160, Last File Parsed: MaxMun_c_0051328_rois_aal_mat.txt\n",
      "Counter: 170, Last File Parsed: MaxMun_c_0051343_rois_aal_mat.txt\n",
      "Counter: 180, Last File Parsed: MaxMun_d_0051355_rois_aal_mat.txt\n",
      "Counter: 190, Last File Parsed: NYU_0050960_rois_aal_mat.txt\n",
      "Counter: 200, Last File Parsed: NYU_0050972_rois_aal_mat.txt\n",
      "Counter: 210, Last File Parsed: NYU_0050984_rois_aal_mat.txt\n",
      "Counter: 220, Last File Parsed: NYU_0050994_rois_aal_mat.txt\n",
      "Counter: 230, Last File Parsed: NYU_0051006_rois_aal_mat.txt\n",
      "Counter: 240, Last File Parsed: NYU_0051016_rois_aal_mat.txt\n",
      "Counter: 250, Last File Parsed: NYU_0051027_rois_aal_mat.txt\n",
      "Counter: 260, Last File Parsed: NYU_0051040_rois_aal_mat.txt\n",
      "Counter: 270, Last File Parsed: NYU_0051051_rois_aal_mat.txt\n",
      "Counter: 280, Last File Parsed: NYU_0051061_rois_aal_mat.txt\n",
      "Counter: 290, Last File Parsed: NYU_0051071_rois_aal_mat.txt\n",
      "Counter: 300, Last File Parsed: NYU_0051081_rois_aal_mat.txt\n",
      "Counter: 310, Last File Parsed: NYU_0051091_rois_aal_mat.txt\n",
      "Counter: 320, Last File Parsed: NYU_0051102_rois_aal_mat.txt\n",
      "Counter: 330, Last File Parsed: NYU_0051113_rois_aal_mat.txt\n",
      "Counter: 340, Last File Parsed: NYU_0051128_rois_aal_mat.txt\n",
      "Counter: 350, Last File Parsed: NYU_0051152_rois_aal_mat.txt\n",
      "Counter: 360, Last File Parsed: OHSU_0050147_rois_aal_mat.txt\n",
      "Counter: 370, Last File Parsed: OHSU_0050160_rois_aal_mat.txt\n",
      "Counter: 380, Last File Parsed: Olin_0050105_rois_aal_mat.txt\n",
      "Counter: 390, Last File Parsed: Olin_0050119_rois_aal_mat.txt\n",
      "Counter: 400, Last File Parsed: Olin_0050133_rois_aal_mat.txt\n",
      "Counter: 410, Last File Parsed: Pitt_0050011_rois_aal_mat.txt\n",
      "Counter: 420, Last File Parsed: Pitt_0050027_rois_aal_mat.txt\n",
      "Counter: 430, Last File Parsed: Pitt_0050038_rois_aal_mat.txt\n",
      "Counter: 440, Last File Parsed: Pitt_0050051_rois_aal_mat.txt\n",
      "Counter: 450, Last File Parsed: SBL_0051559_rois_aal_mat.txt\n",
      "Counter: 460, Last File Parsed: SBL_0051571_rois_aal_mat.txt\n",
      "Counter: 470, Last File Parsed: SBL_0051582_rois_aal_mat.txt\n",
      "Counter: 480, Last File Parsed: SDSU_0050189_rois_aal_mat.txt\n",
      "Counter: 490, Last File Parsed: SDSU_0050200_rois_aal_mat.txt\n",
      "Counter: 500, Last File Parsed: SDSU_0050211_rois_aal_mat.txt\n",
      "Counter: 510, Last File Parsed: Stanford_0051164_rois_aal_mat.txt\n",
      "Counter: 520, Last File Parsed: Stanford_0051175_rois_aal_mat.txt\n",
      "Counter: 530, Last File Parsed: Stanford_0051186_rois_aal_mat.txt\n",
      "Counter: 540, Last File Parsed: Stanford_0051197_rois_aal_mat.txt\n",
      "Counter: 550, Last File Parsed: Trinity_0050240_rois_aal_mat.txt\n",
      "Counter: 560, Last File Parsed: Trinity_0050252_rois_aal_mat.txt\n",
      "Counter: 570, Last File Parsed: Trinity_0050264_rois_aal_mat.txt\n",
      "Counter: 580, Last File Parsed: Trinity_0051135_rois_aal_mat.txt\n",
      "Counter: 590, Last File Parsed: UCLA_1_0051210_rois_aal_mat.txt\n",
      "Counter: 600, Last File Parsed: UCLA_1_0051223_rois_aal_mat.txt\n",
      "Counter: 610, Last File Parsed: UCLA_1_0051239_rois_aal_mat.txt\n",
      "Counter: 620, Last File Parsed: UCLA_1_0051255_rois_aal_mat.txt\n",
      "Counter: 630, Last File Parsed: UCLA_1_0051268_rois_aal_mat.txt\n",
      "Counter: 640, Last File Parsed: UCLA_1_0051281_rois_aal_mat.txt\n",
      "Counter: 650, Last File Parsed: UCLA_2_0051305_rois_aal_mat.txt\n",
      "Counter: 660, Last File Parsed: UCLA_2_0051316_rois_aal_mat.txt\n",
      "Counter: 670, Last File Parsed: UM_1_0050282_rois_aal_mat.txt\n",
      "Counter: 680, Last File Parsed: UM_1_0050294_rois_aal_mat.txt\n",
      "Counter: 690, Last File Parsed: UM_1_0050315_rois_aal_mat.txt\n",
      "Counter: 700, Last File Parsed: UM_1_0050330_rois_aal_mat.txt\n",
      "Counter: 710, Last File Parsed: UM_1_0050341_rois_aal_mat.txt\n",
      "Counter: 720, Last File Parsed: UM_1_0050351_rois_aal_mat.txt\n",
      "Counter: 730, Last File Parsed: UM_1_0050363_rois_aal_mat.txt\n",
      "Counter: 740, Last File Parsed: UM_1_0050374_rois_aal_mat.txt\n",
      "Counter: 750, Last File Parsed: UM_2_0050391_rois_aal_mat.txt\n",
      "Counter: 760, Last File Parsed: UM_2_0050411_rois_aal_mat.txt\n",
      "Counter: 770, Last File Parsed: UM_2_0050424_rois_aal_mat.txt\n",
      "Counter: 780, Last File Parsed: USM_0050438_rois_aal_mat.txt\n",
      "Counter: 790, Last File Parsed: USM_0050448_rois_aal_mat.txt\n",
      "Counter: 800, Last File Parsed: USM_0050482_rois_aal_mat.txt\n",
      "Counter: 810, Last File Parsed: USM_0050494_rois_aal_mat.txt\n",
      "Counter: 820, Last File Parsed: USM_0050507_rois_aal_mat.txt\n",
      "Counter: 830, Last File Parsed: USM_0050526_rois_aal_mat.txt\n",
      "Counter: 840, Last File Parsed: Yale_0050555_rois_aal_mat.txt\n",
      "Counter: 850, Last File Parsed: Yale_0050566_rois_aal_mat.txt\n",
      "Counter: 860, Last File Parsed: Yale_0050576_rois_aal_mat.txt\n",
      "Counter: 870, Last File Parsed: Yale_0050612_rois_aal_mat.txt\n",
      "Counter: 880, Last File Parsed: Yale_0050625_rois_aal_mat.txt\n",
      "Average Degree NT: 34.216564614888824\n",
      "Average Degree ASD: 34.12766517504606\n"
     ]
    }
   ],
   "source": [
    "# Let's get the average degrees of all our things! this sucks man\n",
    "import os\n",
    "matrixDirectory = R\"C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output_AAL\\30%\\Matrices\"\n",
    "outputDirectory = R\"C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output_AAL\\30%\\NetworkMetrics\"\n",
    "fileNames = os.listdir(matrixDirectory)\n",
    "\n",
    "masterSpreadsheet = pd.read_csv(\"master_data_cc400.csv\")\n",
    "# print(masterSpreadsheet.head())\n",
    "counter = 0\n",
    "# For each matrix \n",
    "avg_degrees_asd = []\n",
    "avg_degrees_nt = []\n",
    "for matrixFileName in fileNames:\n",
    "    # print(matrixFileName)\n",
    "    strippedFileName = matrixFileName.split('_rois')[0]\n",
    "    # print(strippedFileName)\n",
    "    if counter % 10 == 0:\n",
    "        print(f\"Counter: {counter}, Last File Parsed: {matrixFileName}\")\n",
    "    \n",
    "    # Load the matrix\n",
    "    matrix = np.loadtxt(f\"{matrixDirectory}\\{matrixFileName}\")\n",
    "    # Calculate network metrics\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    averageDegree = np.mean([G.degree(node) for node in G])\n",
    "    participant = masterSpreadsheet[masterSpreadsheet['Participant ID'] == strippedFileName]\n",
    "    \n",
    "    if not participant.empty:\n",
    "        # print(participant.iloc[0]['DX_Group'])\n",
    "        if (participant.iloc[0]['DX_Group'] == 1):\n",
    "            avg_degrees_asd.append(averageDegree)\n",
    "        else:\n",
    "            avg_degrees_nt.append(averageDegree)\n",
    "    \n",
    "    # print(masterSpreadsheet[masterSpreadsheet['Participant ID'] == strippedFileName])\n",
    "    # avg_degrees.append(averageDegree)\n",
    "    # networkMetrics = CalculateNetworkMetrics(matrix)\n",
    "    # Output network metrics csv\n",
    "    # OutputGraphMetrics(networkMetrics, outputDirectory, matrixFileName)\n",
    "    # print(networkMetrics)\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "# # print(np.mean(avg_degrees))\n",
    "print(f\"Average Degree NT: {np.mean(avg_degrees_nt)}\")\n",
    "print(f\"Average Degree ASD: {np.mean(avg_degrees_asd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_O' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(nx\u001b[38;5;241m.\u001b[39mglobal_efficiency(\u001b[43mG_O\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(nx\u001b[38;5;241m.\u001b[39mglobal_efficiency(G_R))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg Clustering\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'G_O' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(nx.global_efficiency(G_O))\n",
    "print(nx.global_efficiency(G_R))\n",
    "\n",
    "print(\"Avg Clustering\")\n",
    "print(nx.average_clustering(G_O))\n",
    "print(nx.average_clustering(G_R))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
