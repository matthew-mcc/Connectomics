{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "from nilearn.image import load_img\n",
    "from nilearn.plotting import find_parcellation_cut_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\mattm\\AppData\\Local\\Temp\\ipykernel_6356\\1127828871.py:5: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  fullPath = f\"{dir}\\{fileName}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def FileToMatrix(fileName, dir):\n",
    "    '''\n",
    "    Function to load a particular time series, and generate a matrix for it!\n",
    "    '''\n",
    "    fullPath = f\"{dir}\\{fileName}\"\n",
    "    data = np.loadtxt(fullPath)\n",
    "    correlationMatrix = np.corrcoef(data.T) # need to transpose (I think for all?)\n",
    "    np.fill_diagonal(correlationMatrix, 0) # remove leading zeros\n",
    "    \n",
    "    return correlationMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadAtlas(fileName):\n",
    "    '''\n",
    "    Simple function to load an atlas, and return coords.\n",
    "    '''\n",
    "    \n",
    "    atlasImg = load_img(fileName)\n",
    "    coords = find_parcellation_cut_coords(atlasImg)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PercentageThresholding(correlationMatrix, percentageToKeep):\n",
    "    '''\n",
    "    Function to apply a percentage based thresholding, returns new adjacency matrix\n",
    "    '''\n",
    "    \n",
    "    # Apply Fisher z-transformation to the correlation matrix\n",
    "    transformedMatrix = np.arctanh(correlationMatrix)\n",
    "    \n",
    "    # Flatten the matrix, sort the values in descending order\n",
    "    sortedValues = np.sort(transformedMatrix.flatten())[::-1]\n",
    "    \n",
    "    # Determine the thresholded value based on percentage (how many elements to keep)\n",
    "    numElementsToKeep = int(len(sortedValues) * percentageToKeep)\n",
    "    thresholdValue = sortedValues[numElementsToKeep - 1]\n",
    "    \n",
    "    # Create a new adjacency matrix based on our thresholding\n",
    "    adjMatrix = np.zeros(transformedMatrix.shape, dtype=int) # int for binary\n",
    "    adjMatrix[transformedMatrix >= thresholdValue] = 1 # 1 if keeping, 0 if thresholded\n",
    "    \n",
    "\n",
    "    \n",
    "    return adjMatrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputBrainNetViewer(nodeData, G, coords, outputNodeName, outputEdgeName, weighted=False):\n",
    "    '''\n",
    "    Formats data for Brain Net Viewer.\n",
    "    '''\n",
    "    # Add coordinates from atlas\n",
    "    \n",
    "    if weighted:\n",
    "        weightedDegrees = dict(G.degree(weight='weight'))\n",
    "    else:\n",
    "        weightedDegrees = dict(G.degree())\n",
    "    \n",
    "\n",
    "    nodeData['roi'] = nodeData.index\n",
    "    \n",
    "    nodeData['x'] = [coord[0] for coord in coords]\n",
    "    nodeData['y'] = [coord[1] for coord in coords]\n",
    "    nodeData['z'] = [coord[2] for coord in coords]\n",
    "    \n",
    "    nodeData['weightedDegree'] = nodeData.index.map(weightedDegrees)\n",
    "\n",
    "    # Save node file for Brain Net Viewer\n",
    "    nodeData[['x', 'y', 'z', 'roi', 'weightedDegree']].to_csv(outputNodeName, sep='\\t', index=False, header=False)\n",
    "\n",
    "    # Save adjacency matrix as .edge file\n",
    "    edge_matrix = nx.to_numpy_array(G)\n",
    "    np.savetxt(outputEdgeName, edge_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputMatrix(matrix, dir, fileName):\n",
    "    # Caltech_0051456_rois_cc400.1D\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    \n",
    "    # Modify the filename to end with \"_mat.txt\"\n",
    "    baseName = fileName.split('.')[0]\n",
    "    outputFileName = f\"{baseName}_mat.txt\"\n",
    "    \n",
    "    # Construct the full path for the output file\n",
    "    outputPath = os.path.join(dir, outputFileName)\n",
    "    \n",
    "    # Save the matrix as a text file\n",
    "    np.savetxt(outputPath, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Acquisition__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Last File Parsed: Caltech_0051456_rois_cc400.1D\n",
      "Counter: 10, Last File Parsed: Caltech_0051466_rois_cc400.1D\n",
      "Counter: 20, Last File Parsed: Caltech_0051477_rois_cc400.1D\n",
      "Counter: 30, Last File Parsed: Caltech_0051487_rois_cc400.1D\n",
      "Counter: 40, Last File Parsed: CMU_b_0050657_rois_cc400.1D\n",
      "Counter: 50, Last File Parsed: KKI_0050780_rois_cc400.1D\n",
      "Counter: 60, Last File Parsed: KKI_0050791_rois_cc400.1D\n",
      "Counter: 70, Last File Parsed: KKI_0050814_rois_cc400.1D\n",
      "Counter: 80, Last File Parsed: KKI_0050825_rois_cc400.1D\n",
      "Counter: 90, Last File Parsed: Leuven_1_0050692_rois_cc400.1D\n",
      "Counter: 100, Last File Parsed: Leuven_1_0050702_rois_cc400.1D\n",
      "Counter: 110, Last File Parsed: Leuven_2_0050722_rois_cc400.1D\n",
      "Counter: 120, Last File Parsed: Leuven_2_0050733_rois_cc400.1D\n",
      "Counter: 130, Last File Parsed: Leuven_2_0050745_rois_cc400.1D\n",
      "Counter: 140, Last File Parsed: Leuven_2_0050756_rois_cc400.1D\n",
      "Counter: 150, Last File Parsed: MaxMun_a_0051370_rois_cc400.1D\n",
      "Counter: 160, Last File Parsed: MaxMun_c_0051328_rois_cc400.1D\n",
      "Counter: 170, Last File Parsed: MaxMun_c_0051343_rois_cc400.1D\n",
      "Counter: 180, Last File Parsed: MaxMun_d_0051355_rois_cc400.1D\n",
      "Counter: 190, Last File Parsed: NYU_0050960_rois_cc400.1D\n",
      "Counter: 200, Last File Parsed: NYU_0050972_rois_cc400.1D\n",
      "Counter: 210, Last File Parsed: NYU_0050984_rois_cc400.1D\n",
      "Counter: 220, Last File Parsed: NYU_0050994_rois_cc400.1D\n",
      "Counter: 230, Last File Parsed: NYU_0051006_rois_cc400.1D\n",
      "Counter: 240, Last File Parsed: NYU_0051016_rois_cc400.1D\n",
      "Counter: 250, Last File Parsed: NYU_0051027_rois_cc400.1D\n",
      "Counter: 260, Last File Parsed: NYU_0051040_rois_cc400.1D\n",
      "Counter: 270, Last File Parsed: NYU_0051051_rois_cc400.1D\n",
      "Counter: 280, Last File Parsed: NYU_0051061_rois_cc400.1D\n",
      "Counter: 290, Last File Parsed: NYU_0051071_rois_cc400.1D\n",
      "Counter: 300, Last File Parsed: NYU_0051081_rois_cc400.1D\n",
      "Counter: 310, Last File Parsed: NYU_0051091_rois_cc400.1D\n",
      "Counter: 320, Last File Parsed: NYU_0051102_rois_cc400.1D\n",
      "Counter: 330, Last File Parsed: NYU_0051113_rois_cc400.1D\n",
      "Counter: 340, Last File Parsed: NYU_0051128_rois_cc400.1D\n",
      "Counter: 350, Last File Parsed: NYU_0051152_rois_cc400.1D\n",
      "Counter: 360, Last File Parsed: OHSU_0050147_rois_cc400.1D\n",
      "Counter: 370, Last File Parsed: OHSU_0050160_rois_cc400.1D\n",
      "Counter: 380, Last File Parsed: Olin_0050105_rois_cc400.1D\n",
      "Counter: 390, Last File Parsed: Olin_0050119_rois_cc400.1D\n",
      "Counter: 400, Last File Parsed: Olin_0050133_rois_cc400.1D\n",
      "Counter: 410, Last File Parsed: Pitt_0050011_rois_cc400.1D\n",
      "Counter: 420, Last File Parsed: Pitt_0050027_rois_cc400.1D\n",
      "Counter: 430, Last File Parsed: Pitt_0050038_rois_cc400.1D\n",
      "Counter: 440, Last File Parsed: Pitt_0050051_rois_cc400.1D\n",
      "Counter: 450, Last File Parsed: SBL_0051559_rois_cc400.1D\n",
      "Counter: 460, Last File Parsed: SBL_0051571_rois_cc400.1D\n",
      "Counter: 470, Last File Parsed: SBL_0051582_rois_cc400.1D\n",
      "Counter: 480, Last File Parsed: SDSU_0050189_rois_cc400.1D\n",
      "Counter: 490, Last File Parsed: SDSU_0050200_rois_cc400.1D\n",
      "Counter: 500, Last File Parsed: SDSU_0050211_rois_cc400.1D\n",
      "Counter: 510, Last File Parsed: Stanford_0051164_rois_cc400.1D\n",
      "Counter: 520, Last File Parsed: Stanford_0051175_rois_cc400.1D\n",
      "Counter: 530, Last File Parsed: Stanford_0051186_rois_cc400.1D\n",
      "Counter: 540, Last File Parsed: Stanford_0051197_rois_cc400.1D\n",
      "Counter: 550, Last File Parsed: Trinity_0050240_rois_cc400.1D\n",
      "Counter: 560, Last File Parsed: Trinity_0050252_rois_cc400.1D\n",
      "Counter: 570, Last File Parsed: Trinity_0050264_rois_cc400.1D\n",
      "Counter: 580, Last File Parsed: Trinity_0051135_rois_cc400.1D\n",
      "Counter: 590, Last File Parsed: UCLA_1_0051210_rois_cc400.1D\n",
      "Counter: 600, Last File Parsed: UCLA_1_0051223_rois_cc400.1D\n",
      "Counter: 610, Last File Parsed: UCLA_1_0051239_rois_cc400.1D\n",
      "Counter: 620, Last File Parsed: UCLA_1_0051255_rois_cc400.1D\n",
      "Counter: 630, Last File Parsed: UCLA_1_0051268_rois_cc400.1D\n",
      "Counter: 640, Last File Parsed: UCLA_1_0051281_rois_cc400.1D\n",
      "Counter: 650, Last File Parsed: UCLA_2_0051305_rois_cc400.1D\n",
      "Counter: 660, Last File Parsed: UCLA_2_0051316_rois_cc400.1D\n",
      "Counter: 670, Last File Parsed: UM_1_0050282_rois_cc400.1D\n",
      "Counter: 680, Last File Parsed: UM_1_0050294_rois_cc400.1D\n",
      "Counter: 690, Last File Parsed: UM_1_0050315_rois_cc400.1D\n",
      "Counter: 700, Last File Parsed: UM_1_0050330_rois_cc400.1D\n",
      "Counter: 710, Last File Parsed: UM_1_0050341_rois_cc400.1D\n",
      "Counter: 720, Last File Parsed: UM_1_0050351_rois_cc400.1D\n",
      "Counter: 730, Last File Parsed: UM_1_0050363_rois_cc400.1D\n",
      "Counter: 740, Last File Parsed: UM_1_0050374_rois_cc400.1D\n",
      "Counter: 750, Last File Parsed: UM_2_0050391_rois_cc400.1D\n",
      "Counter: 760, Last File Parsed: UM_2_0050411_rois_cc400.1D\n",
      "Counter: 770, Last File Parsed: UM_2_0050424_rois_cc400.1D\n",
      "Counter: 780, Last File Parsed: USM_0050438_rois_cc400.1D\n",
      "Counter: 790, Last File Parsed: USM_0050448_rois_cc400.1D\n",
      "Counter: 800, Last File Parsed: USM_0050482_rois_cc400.1D\n",
      "Counter: 810, Last File Parsed: USM_0050494_rois_cc400.1D\n",
      "Counter: 820, Last File Parsed: USM_0050507_rois_cc400.1D\n",
      "Counter: 830, Last File Parsed: USM_0050526_rois_cc400.1D\n",
      "Counter: 840, Last File Parsed: Yale_0050555_rois_cc400.1D\n",
      "Counter: 850, Last File Parsed: Yale_0050566_rois_cc400.1D\n",
      "Counter: 860, Last File Parsed: Yale_0050576_rois_cc400.1D\n",
      "Counter: 870, Last File Parsed: Yale_0050612_rois_cc400.1D\n",
      "Counter: 880, Last File Parsed: Yale_0050625_rois_cc400.1D\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataDirectory = R\"C:\\GIT\\Connectomics\\Research_Project\\Data\\ALL\\Outputs\\ccs\\nofilt_noglobal\\rois_cc400\"\n",
    "fileNames = os.listdir(dataDirectory)\n",
    "# print(fileNames)\n",
    "\n",
    "\n",
    "# For each data point\n",
    "counter = 0\n",
    "for fileName in fileNames:\n",
    "    \n",
    "    if counter % 10 == 0:\n",
    "        print(f\"Counter: {counter}, Last File Parsed: {fileName}\")\n",
    "    \n",
    "    correlationMat = FileToMatrix(fileName, dataDirectory)\n",
    "    adjacencyMat = PercentageThresholding(correlationMat, 0.30)\n",
    "    # OutputMatrix(adjacencyMat, R\"Binary_Output/30%/Matrices\", fileName) <-- Commented to ensure our data is not overwritten\n",
    "    \n",
    "    counter+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Nodes to Brain #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of our correlation matrices: \n",
    "\n",
    "# 1. Create a \".node / .edge\" file, mapped to the Craddock CC400 atlas\n",
    "\n",
    "# 1.1 This will give us a mapping to which nodes matter for intelligence, as we have various regions. This also gives us a map for determining the significance of our graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analysis #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "def OutputGraphMetrics(data, dir, fileName):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    \n",
    "    # Remove _mat.txt\n",
    "    fileName = fileName[:-8]\n",
    "    # Modify the filename to end with a particular extension\n",
    "    fileName = f\"{fileName}_metrics.csv\"\n",
    "    # Construct the full path for the output file\n",
    "    outputPath = os.path.join(dir, fileName)\n",
    "    \n",
    "    # Write data to a CSV file\n",
    "    with open(outputPath, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Metric\", \"Value\"])  # Write header\n",
    "        \n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Convert dictionary to string for storage\n",
    "                value_str = \"; \".join([f\"{k}: {v}\" for k, v in value.items()])\n",
    "                writer.writerow([key, value_str])\n",
    "            else:\n",
    "                writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As described in 2.5.3.1 in the Hilger Paper\n",
    "def CalculateNodalEfficiency(G):\n",
    "    '''\n",
    "    Calculates Nodal Efficiency for each node in a graph:\n",
    "        - Where, Nodal Efficiency is defined as inversely proportional to the average shortest distance between node i and all other nodes j of the graph.\n",
    "    '''\n",
    "    nodalEfficiency = {}\n",
    "    N = G.number_of_nodes()\n",
    "    for i in G.nodes:\n",
    "        # Calculate the shortest path lengths from node i to all other nodes\n",
    "        shortestPaths = nx.single_source_shortest_path_length(G, i)\n",
    "        \n",
    "        # Calculate nodal efficiency for node i\n",
    "        efficiencySum = sum(1 / shortestPaths[j] for j in shortestPaths if j != i and shortestPaths[j] > 0)\n",
    "        nodalEfficiency[i] = efficiencySum / (N - 1)\n",
    "        \n",
    "    return nodalEfficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note on Multiple Thresholding__\n",
    "\n",
    "This approach follows the Hilger paper. In essence, we will calculate network metrics for each of the 5 thresholding levels, and then average these network metrics together to be used in analysis. An alternative approach might be to average the correlation matrices, but the former seems more reasonable at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of our correlation matrices:\n",
    "# 1. Create a Graph\n",
    "matrix = np.loadtxt(\"Binary_Output/10%/Matrices/Caltech_0051456_rois_cc400_mat.txt\")\n",
    "G = nx.from_numpy_array(matrix)\n",
    "\n",
    "# ? Average ?\n",
    "# 2. Calculate Graph Metrics:\n",
    "\n",
    "# 2.1 Nodal Efficiency (as described by hilger paper)\n",
    "nodalEfficiency = CalculateNodalEfficiency(G)\n",
    "# 2.2 Global Efficiency \n",
    "globalEfficiency = nx.global_efficiency(G)\n",
    "# 2.3 Degree Centrality <-- For Each Node\n",
    "degreeCentrality = nx.degree_centrality(G)\n",
    "# 2.4 Average Shortest Path\n",
    "averageShortestPathLength = 0\n",
    "if nx.is_connected(G):\n",
    "    averageShortestPathLength = nx.average_shortest_path_length(G)\n",
    "# 2.5 Average Degree\n",
    "averageDegree = np.mean([G.degree(node) for node in G])\n",
    "# 2.6 Average Clustering Coefficient\n",
    "averageClustering = nx.average_clustering(G)\n",
    "# 2.6 Number of Nodes and Edges\n",
    "numNodes = G.number_of_nodes()\n",
    "numEdges = G.number_of_edges()\n",
    "\n",
    "data = {\n",
    "    \"Nodal Efficiency\": nodalEfficiency,  # Dictionary format\n",
    "    \"Global Efficiency\": globalEfficiency,\n",
    "    \"Degree Centrality\": degreeCentrality,  # Dictionary format\n",
    "    \"Average Shortest Path Length\": averageShortestPathLength,\n",
    "    \"Average Degree\": averageDegree,\n",
    "    \"Average Clustering Coefficient\": averageClustering,\n",
    "    \"Number of Nodes\": numNodes,\n",
    "    \"Number of Edges\": numEdges\n",
    "}\n",
    "\n",
    "dataDirectory = R\"C:\\GIT\\Connectomics\\Research_Project\\Binary_Notebooks\\Binary_Output\\10%\\NetworkMetrics\"\n",
    "fileName = \"Caltech_0051456_rois_cc400_mat.txt\"\n",
    "\n",
    "OutputGraphMetrics(data, dataDirectory, fileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
